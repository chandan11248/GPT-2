{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6e0f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc.txt',\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_data=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98cf4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c89aa4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "enc_text=tokenizer.encode(raw_data)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35502746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8582, 234, 247, 383]--->6869\n"
     ]
    }
   ],
   "source": [
    "context_length=4\n",
    "x=enc_text[:context_length]\n",
    "y=enc_text[context_length]\n",
    "print(f\"{x}--->{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebc2e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½ ---> ï¿½\n",
      "ï¿½ ---> ï¿½\n",
      "ðŸŒ™ --->  The\n",
      "ðŸŒ™ The --->  Moon\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,context_length+1):\n",
    "    context=enc_text[:i]\n",
    "    desired=enc_text[i]\n",
    "    print(tokenizer.decode(context),\"--->\",tokenizer.decode([desired]))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6a9cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GPTdatasetv1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunks = token_ids[i : i + max_length]\n",
    "            target_chunks = token_ids[i + 1 : i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunks, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(target_chunks, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e460fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "def create_dataloader_v1(\n",
    "        txt,\n",
    "        batch_size=4,\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTdatasetv1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a16ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS\n",
      " tensor([[8582,  234,  247,  383],\n",
      "        [ 234,  247,  383, 6869],\n",
      "        [ 247,  383, 6869,   12],\n",
      "        [ 383, 6869,   12,   50],\n",
      "        [6869,   12,   50, 2287],\n",
      "        [  12,   50, 2287,  276],\n",
      "        [  50, 2287,  276, 6785],\n",
      "        [2287,  276, 6785,  851]])\n",
      "targets\n",
      " tensor([[ 234,  247,  383, 6869],\n",
      "        [ 247,  383, 6869,   12],\n",
      "        [ 383, 6869,   12,   50],\n",
      "        [6869,   12,   50, 2287],\n",
      "        [  12,   50, 2287,  276],\n",
      "        [  50, 2287,  276, 6785],\n",
      "        [2287,  276, 6785,  851],\n",
      "        [ 276, 6785,  851,  317]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader=create_dataloader_v1(raw_data,batch_size=8,max_length=4,stride=1,shuffle=False)\n",
    "data_iter=iter (dataloader)\n",
    "inputs,target=next(data_iter)\n",
    "print(\"INPUTS\\n\",inputs)\n",
    "print(\"targets\\n\",target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e4b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fbd07a",
   "metadata": {},
   "source": [
    ">token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3294c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  1.3337,  0.0771, -0.0522],\n",
      "        [ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [-1.3152, -0.0677, -0.1350,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        ...,\n",
      "        [ 0.5871, -0.0572, -1.1628,  ..., -0.6887, -0.7364,  0.4479],\n",
      "        [ 0.4438,  0.7411,  1.1263,  ...,  1.2091,  0.6781,  0.3331],\n",
      "        [-0.2537,  0.1446,  0.7203,  ..., -0.2134,  0.2144,  0.3006]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vocab_size=50257\n",
    "output_dim=256\n",
    "torch.manual_seed(123)  #random value alwase be constant when it is run \n",
    "embedding_layer=torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0d15058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [ 0.5229,  0.1553,  0.5247,  ..., -0.2004,  0.8093, -0.6667],\n",
      "        [-0.3162,  1.2700, -0.0903,  ..., -0.4098,  0.4978, -0.3721],\n",
      "        [ 0.5229,  0.1553,  0.5247,  ..., -0.2004,  0.8093, -0.6667],\n",
      "        [ 0.2579,  0.3420, -0.8168,  ..., -0.4840, -0.2713, -0.0774],\n",
      "        [-0.3162,  1.2700, -0.0903,  ..., -0.4098,  0.4978, -0.3721]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids=torch.tensor([1,4,5,4,3,5])\n",
    "print(embedding_layer(input_ids)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "badd6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(raw_data,batch_size=8,max_length=max_length,stride=1,shuffle=False)\n",
    "data_iter=iter (dataloader)\n",
    "inputs,target=next(data_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fbaf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS\n",
      " tensor([[8582,  234,  247,  383],\n",
      "        [ 234,  247,  383, 6869],\n",
      "        [ 247,  383, 6869,   12],\n",
      "        [ 383, 6869,   12,   50],\n",
      "        [6869,   12,   50, 2287],\n",
      "        [  12,   50, 2287,  276],\n",
      "        [  50, 2287,  276, 6785],\n",
      "        [2287,  276, 6785,  851]])\n",
      "shape\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUTS\\n\",inputs)\n",
    "print(\"shape\\n\",inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834c269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bddd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings=embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1998309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length=max_length\n",
    "pos_embedding_layer=torch.nn.Embedding(context_length,output_dim)\n",
    "pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bbaed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563bf27",
   "metadata": {},
   "source": [
    "# IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e34e0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "351286e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "267ee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10c8f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out),\n",
    "requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out),\n",
    "requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e34b7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)\n",
    "\n",
    "print(W_key)\n",
    "\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60aa867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2=x_2@W_query\n",
    "key_2=x_2@W_key\n",
    "value_2=x_2@W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3622f2d",
   "metadata": {},
   "source": [
    ">for all input \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07335384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7362d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa9ba654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights = torch.softmax(attn_scores/ d_k**0.5, dim=-1)\n",
    "print(attn_weights)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31c40c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8346, 4.9351],\n",
      "        [2.5091, 6.7530],\n",
      "        [2.4766, 6.6657],\n",
      "        [1.3772, 3.7065],\n",
      "        [1.1948, 3.2184],\n",
      "        [1.7832, 4.7978]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_vec = attn_scores @ values\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261655d",
   "metadata": {},
   "source": [
    "# lets wrap the entire attention mechanism in a class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bdcca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d061d",
   "metadata": {},
   "source": [
    "\tâ€¢\tsuper() returns a special object that represents the parent class.\n",
    "\tâ€¢\tThen super().__init__() calls the parent classâ€™s __init__.\n",
    "\tâ€¢\tThis avoids writing the parent class name manually \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02283fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5805f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#GPT-2's original implementation doesn't use bias for Q/K/V\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7e1af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#likely used 789 to get different initial weights than sa_v1 (which used seed 123), so the outputs would differ.\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f12cb",
   "metadata": {},
   "source": [
    "# masked attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d39f68fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2705, 1.8524,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2544, 1.8284, 1.7877,   -inf,   -inf,   -inf],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925,   -inf,   -inf],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707,   -inf],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f7b87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8dc21",
   "metadata": {},
   "source": [
    "# dropout implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "476fc75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9329b8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.dropout(example,p=0.5,train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04306b35",
   "metadata": {},
   "source": [
    "During training, dropout is active. During inference, dropout is off. If we didn't scale, the model would see different magnitude values at train vs test time, breaking predictions.\n",
    "\n",
    "With scaling, no adjustment is needed at inference â€” the expected values match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "583215da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5052, 0.7582, 0.7366, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5677, 0.5587, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4727, 0.0000, 0.3639, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4183, 0.4097, 0.2839, 0.2178, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d4ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a755013",
   "metadata": {},
   "source": [
    "# batch demonstration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85a6804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a52ea0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628ae6b",
   "metadata": {},
   "source": [
    "# multi_head   attention  with weights split technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e1876",
   "metadata": {},
   "source": [
    ">d_in=input enbedding token dimension \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fb0f0",
   "metadata": {},
   "source": [
    "What is register_buffer?\n",
    "\n",
    "register_buffer is a method in nn.Module that registers a tensor as a buffer in the module.\n",
    "Buffers are tensors that are not learnable parameters (i.e., they are not updated during backpropagation) but are part of the model and saved with it (e.g., during state_dict()).\n",
    "Why use register_buffer?\n",
    "\n",
    "The mask is a fixed tensor that does not require gradients, so it is registered as a buffer instead of a parameter.\n",
    "This ensures the mask is moved to the correct device (CPU/GPU) along with the model, and it is included in the model's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca6df3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"sqb\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "  #self.mask.bool(): Converts the mask to a boolean tensor, where 1 becomes True and 0 becomes False.\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        #masked_fill:Replaces the positions where the mask is True with -torch.inf (negative infinity), effectively preventing attention to those positions.\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    " \n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "   \n",
    "    # This means the context vector has the same \n",
    "    # number of tokens as the input, but it now contains aggregated information from all attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29a65c",
   "metadata": {},
   "source": [
    "Why No Transpose of values?\n",
    "In matrix multiplication, the dot product is performed between the last dimension of the first tensor and the second-to-last dimension of the second tensor. In this case:\n",
    "\n",
    "attn_weights has shape (batch_size, num_heads, num_tokens, num_tokens).\n",
    "values has shape (batch_size, num_heads, num_tokens, head_dim).\n",
    "The dot product attn_weights @ values works as follows:\n",
    "\n",
    "For each batch_size and num_heads, the num_tokens dimension of attn_weights (last dimension) is multiplied with the num_tokens dimension of values (second-to-last dimension).\n",
    "This results in a tensor of shape (batch_size, num_heads, num_tokens, head_dim).\n",
    "Since the dimensions already align for the dot product, there is no need to transpose values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5fa9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5dfed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a370f5",
   "metadata": {},
   "source": [
    "##  A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e45c9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e558fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4cadcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init(self,config):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(num_embeddings=config[\"vocab_size\"],embedding_dim=config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.dropout=nn.Dropout(p=config[\"drop_rate\"])\n",
    "\n",
    "    # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec70b6",
   "metadata": {},
   "source": [
    "# CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "982df416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)#keepdim make seperate mean of each separate \n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bc0c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f0dcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb856b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Layer normalization (LayerNorm) is applied before each of these two components, and\n",
    "dropout is applied after them to regularize the model and prevent overfitting. \n",
    "\n",
    "This is also known as Pre-LayerNorm. \n",
    "\n",
    "Older architectures, such as the original transformer model,\n",
    "applied layer normalization after the self-attention and feed-forward networks instead,\n",
    "known as Post-LayerNorm, which often leads to worse training dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c08185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e81a00",
   "metadata": {},
   "source": [
    "As we can see from the code output, the transformer block maintains the input dimensions in its output, indicating that the transformer architecture processes sequences of data without altering their shape throughout the network.\n",
    "\n",
    "The preservation of shape throughout the transformer block architecture is not incidental but a crucial aspect of its design.\n",
    "\n",
    "This design enables its effective application across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector, maintaining a one-to-one relationship.\n",
    "\n",
    "However, the output is a context vector that encapsulates information from the entire input sequence.\n",
    "\n",
    "This means that while the physical dimensions of the sequence (length and feature size) remain unchanged as it passes through the transformer block, the content of each output vector is re-encoded to integrate contextual information from across the entire input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d625e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input indices:\n",
      " tensor([[8582,  234,  247,  383],\n",
      "        [ 234,  247,  383, 6869]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.4761, -0.6968,  0.2739,  ...,  0.0219,  0.1745, -0.2928],\n",
      "         [-0.0046, -0.4494,  0.7564,  ..., -0.2530, -0.3361, -0.0542],\n",
      "         [ 0.9064,  0.1895,  0.0884,  ...,  0.1766, -0.2653,  0.0426],\n",
      "         [-0.2799, -0.5785,  1.0722,  ...,  0.5362,  0.4676, -0.5018]],\n",
      "\n",
      "        [[-0.8710, -0.1972,  0.1867,  ..., -0.3878,  0.4478, -0.7566],\n",
      "         [ 0.4299, -0.1356, -0.3789,  ...,  0.0278,  0.6706, -0.1964],\n",
      "         [ 1.1687, -0.3557,  0.3495,  ...,  0.0289,  0.6227, -1.0769],\n",
      "         [-0.0725,  0.0998, -0.1130,  ...,  1.1263, -0.7089,  0.1030]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a batch of integer token indices for GPTModel\n",
    "# Example: use the first 4 tokens from enc_text for each batch\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "input_indices = torch.tensor([enc_text[:seq_len], enc_text[1:seq_len+1]])  # shape: (2, 4)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(input_indices)\n",
    "print(\"Input indices:\\n\", input_indices)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5969bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
