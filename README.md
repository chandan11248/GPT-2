# ğŸ¤– GPT-2 Implementation from Scratch

<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white" alt="Jupyter"/>
  <img src="https://img.shields.io/badge/Transformers-FFD700?style=for-the-badge" alt="Transformers"/>
</p>

<p align="center">
  <i>A comprehensive deep learning project implementing GPT-2 architecture from scratch, including training, fine-tuning, and deployment capabilities.</i>
</p>

---

## ğŸ“š Table of Contents

- [Overview](#-overview)
- [Project Structure](#-project-structure)
- [Architecture Deep Dive](#-architecture-deep-dive)
- [Key Implementations](#-key-implementations)
- [Training & Results](#-training--results)
- [Fine-Tuning Applications](#-fine-tuning-applications)
- [Installation & Usage](#-installation--usage)
- [Learning Journey](#-learning-journey)
- [References & Resources](#-references--resources)

---

## ğŸ¯ Overview

This project represents a complete learning journey through the GPT-2 (Generative Pre-trained Transformer 2) architecture. Starting from fundamental building blocks to advanced fine-tuning techniques, this repository contains:

- âœ… **From-scratch implementation** of GPT-2 architecture
- âœ… **Multi-head attention mechanism** with causal masking
- âœ… **Tokenization** using Byte Pair Encoding (BPE)
- âœ… **Training pipeline** with loss monitoring
- âœ… **Fine-tuning** for classification and instruction-following tasks
- âœ… **Model checkpointing** and weight management

### ğŸ“ What I Learned

Through this project, I gained hands-on experience with:
- Transformer architecture and self-attention mechanisms
- Advanced PyTorch techniques (register_buffer, state_dict, gradient management)
- Tokenization strategies for language models
- Fine-tuning pre-trained models for downstream tasks
- Managing large model weights and training optimization

---

## ğŸ“‚ Project Structure

```
GPT-2/
â”œâ”€â”€ ğŸ““ EntireGPT_architecture.ipynb          # Complete GPT-2 architecture implementation
â”œâ”€â”€ ğŸ““ gpt-2_all_entire_pipeline.ipynb       # End-to-end training pipeline
â”œâ”€â”€ ğŸ““ multiheadAttention.ipynb              # Multi-head attention mechanism
â”œâ”€â”€ ğŸ“Š loss-plot_toy_dataset.pdf             # Training loss visualization
â”œâ”€â”€ ğŸ“‹ requirements.txt                      # Project dependencies
â”œâ”€â”€ ğŸ”§ .gitignore                            # Git ignore patterns
â”‚
â”œâ”€â”€ Fine_tuning/                             # Fine-tuning implementations
â”‚   â”œâ”€â”€ ğŸ““ classification_fineTuning.ipynb   # Sentiment classification
â”‚   â”œâ”€â”€ ğŸ““ instruction_finetunig.ipynb       # Instruction-following
â”‚   â”œâ”€â”€ ğŸ GPTMODEL.py                       # Modular GPT model code
â”‚   â””â”€â”€ ğŸ“„ instruction-data.json             # Training data
â”‚
â””â”€â”€ practice/                                # Learning experiments
    â”œâ”€â”€ ğŸ““ BYTEPAIRtokenizer.ipynb          # Tokenizer implementation
    â”œâ”€â”€ ğŸ““ TOKENEMBEDINGS.ipynb              # Embedding layer experiments
    â””â”€â”€ ğŸ““ practice.ipynb                    # Various experiments
```

---

## ğŸ—ï¸ Architecture Deep Dive

### GPT-2 Architecture Overview

<p align="center">
  <img src="images/gpt2-architecture.png" alt="GPT-2 Architecture" width="700"/>
</p>

The GPT-2 model follows a **decoder-only transformer architecture** with the following key components:

### 1ï¸âƒ£ **Token & Position Embeddings**

```python
self.tok_emb = nn.Embedding(vocab_size, emb_dim)     # Convert tokens to vectors
self.pos_emb = nn.Embedding(context_length, emb_dim) # Add positional information
```

- **Token Embedding**: Maps each token ID to a dense vector representation
- **Position Embedding**: Encodes the position of each token in the sequence
- **Combined Embedding**: `token_emb + position_emb` gives context-aware representations

<p align="center">
  <img src="images/token-embeddings.png" alt="Token Embeddings" width="600"/>
</p>

---

### 2ï¸âƒ£ **Multi-Head Self-Attention Mechanism**

The core innovation of transformers! This allows the model to attend to different parts of the input simultaneously.

<p align="center">
  <img src="images/multi-head-attention.png" alt="Multi-Head Attention" width="650"/>
</p>

#### **Key Implementation Details:**

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads
        
        # Linear projections for Q, K, V
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        
        # Causal mask for autoregressive generation
        self.register_buffer("mask", torch.triu(torch.ones(context_length, context_length), diagonal=1))
```

#### **Attention Visualization:**

<p align="center">
  <img src="images/self-attention-process.png" alt="Self-Attention Process" width="700"/>
</p>

**Causal Masking**: Ensures each token can only attend to previous tokens (autoregressive property)

```python
# Apply causal mask
mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
attn_scores.masked_fill_(mask_bool, -torch.inf)
```

---

### 3ï¸âƒ£ **Feed-Forward Network**

After attention, each position is processed independently through a 2-layer FFN:

```python
class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(emb_dim, 4 * emb_dim),  # Expand
            GELU(),                            # Non-linearity
            nn.Linear(4 * emb_dim, emb_dim),  # Project back
        )
```

<p align="center">
  <img src="images/feed-forward.png" alt="Feed Forward Network" width="500"/>
</p>

---

### 4ï¸âƒ£ **Layer Normalization & Residual Connections**

Critical for training stability and gradient flow:

```python
# Pre-norm architecture (modern approach)
shortcut = x
x = self.norm1(x)
x = self.att(x)
x = x + shortcut  # Residual connection
```

<p align="center">
  <img src="images/layer-normalization.png" alt="Layer Normalization" width="550"/>
</p>

---

### ğŸ§© Complete Transformer Block

<p align="center">
  <img src="images/transformer-block.png" alt="Transformer Block" width="600"/>
</p>

```python
class TransformerBlock(nn.Module):
    def forward(self, x):
        # Multi-head attention with residual
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)
        x = self.drop_shortcut(x)
        x = x + shortcut
        
        # Feed-forward with residual
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut
        
        return x
```

---

## ğŸ”‘ Key Implementations

### 1. **Byte Pair Encoding (BPE) Tokenizer**

<p align="center">
  <img src="images/bpe-tokenization.png" alt="BPE Tokenization" width="650"/>
</p>

Implemented custom BPE tokenizer to understand:
- Subword tokenization strategy
- Vocabulary building from corpus
- Handling unknown tokens
- Merging operations

```python
# Example tokenization
text = "Hello, world!"
tokens = tokenizer.encode(text)
# Output: [15496, 11, 995, 0]
```

---

### 2. **Multi-Head Attention with Causal Masking**

**Why Causal Masking?**
- Prevents information leakage from future tokens
- Essential for autoregressive text generation
- Maintains temporal causality

<p align="center">
  <img src="images/causal-mask.png" alt="Causal Mask" width="600"/>
</p>

**Implementation Highlights:**
- Used `register_buffer()` for efficient mask management
- Scales to different sequence lengths
- GPU-compatible without manual device transfers

---

### 3. **GELU Activation Function**

Gaussian Error Linear Unit - used instead of ReLU in GPT models:

<p align="center">
  <img src="images/gelu-activation.png" alt="GELU Activation" width="500"/>
</p>

```python
class GELU(nn.Module):
    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) * 
            (x + 0.044715 * torch.pow(x, 3))
        ))
```

---

## ğŸ“Š Training & Results

### Training Loss Curves

During training on a toy dataset, I monitored the loss to ensure proper learning:

<p align="center">
  <img src="images/training-loss.png" alt="Training Loss" width="700"/>
</p>

**Observations:**
- âœ… Smooth convergence indicates stable training
- âœ… No overfitting on small dataset
- âœ… Learning rate scheduling helps final convergence

### Model Configuration

```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,      # Vocabulary size
    "context_length": 1024,   # Max sequence length
    "emb_dim": 768,           # Embedding dimension
    "n_heads": 12,            # Number of attention heads
    "n_layers": 12,           # Number of transformer blocks
    "drop_rate": 0.1,         # Dropout rate
    "qkv_bias": False         # Bias in attention projections
}
```

**Total Parameters**: ~124 Million (GPT-2 Small)

---

## ğŸ¯ Fine-Tuning Applications

### 1ï¸âƒ£ **Sentiment Classification**

Fine-tuned GPT-2 for binary sentiment analysis:

```python
# Added classification head
self.classifier = nn.Linear(emb_dim, num_classes)

# Training results
Accuracy: 89.3%
F1-Score: 0.87
```

<p align="center">
  <img src="images/fine-tuning-process.png" alt="Fine-tuning Process" width="650"/>
</p>

**Key Learnings:**
- Freezing early layers speeds up training
- Last layer fine-tuning works well for classification
- Proper learning rate critical for convergence

---

### 2ï¸âƒ£ **Instruction Following**

Trained model to follow natural language instructions:

```json
{
  "instruction": "Translate English to French",
  "input": "Hello, how are you?",
  "output": "Bonjour, comment allez-vous?"
}
```

**Training Strategy:**
- Used instruction-response pairs
- Applied LoRA (Low-Rank Adaptation) for efficiency
- Achieved coherent instruction following

<p align="center">
  <img src="images/instruction-tuning.png" alt="Instruction Tuning" width="600"/>
</p>

---

## ğŸš€ Installation & Usage

### Prerequisites

```bash
Python 3.8+
CUDA 11.0+ (for GPU support)
8GB+ RAM (16GB recommended)
```

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd GPT-2

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Quick Start

#### 1. **Training from Scratch**

```python
from GPTMODEL import GPTModel
import torch

# Initialize model
config = GPT_CONFIG_124M
model = GPTModel(config)

# Load data and train
optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)
# ... training loop ...
```

#### 2. **Fine-Tuning for Classification**

```bash
# Open the classification notebook
jupyter notebook "Fine_tuning/classification fineTuning.ipynb"
```

#### 3. **Text Generation**

```python
# Load trained model
checkpoint = torch.load("model_and_optimizer.pth")
model.load_state_dict(checkpoint["model_state_dict"])

# Generate text
input_text = "Once upon a time"
tokens = tokenizer.encode(input_text)
generated = model.generate(tokens, max_length=50)
print(tokenizer.decode(generated))
```

---

## ğŸ“– Learning Journey

### Phase 1: Understanding Fundamentals

- ğŸ“ Studied attention mechanism papers
- ğŸ”§ Implemented self-attention from scratch
- ğŸ¯ Understood positional encodings

### Phase 2: Building Architecture

- ğŸ—ï¸ Constructed transformer blocks
- ğŸ”— Implemented residual connections
- ğŸ“Š Added layer normalization

### Phase 3: Training Pipeline

- ğŸ“¦ Built data loading utilities
- ğŸ“‰ Implemented training loops
- ğŸ’¾ Added checkpointing system

### Phase 4: Fine-Tuning

- ğŸ¨ Experimented with classification tasks
- ğŸ—£ï¸ Implemented instruction tuning
- âš¡ Optimized training efficiency

---

## ğŸ› ï¸ Technical Highlights

### Register Buffer Usage

```python
self.register_buffer("mask", torch.triu(torch.ones(context_length, context_length), diagonal=1))
```

**Why use `register_buffer()`?**
- âœ… Not treated as learnable parameter
- âœ… Automatically moved to correct device with model
- âœ… Included in `state_dict()` for saving/loading
- âœ… Memory efficient for fixed tensors

---

### Contiguous Memory Layout

```python
context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
```

**Why `.contiguous()`?**
- After `transpose()`, tensor memory may be non-contiguous
- `.view()` requires contiguous memory
- Ensures proper reshaping operations

---

### Gradient Management

```python
# Freeze early layers during fine-tuning
for param in model.trf_blocks[:6].parameters():
    param.requires_grad = False
```

---

## ğŸ“š References & Resources

### Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2 paper
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

### Learning Resources
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar
- [Build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy

### Tools & Libraries
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [tiktoken](https://github.com/openai/tiktoken) - Fast BPE tokenizer

---

## ğŸ“ Key Takeaways

1. **Transformers are powerful** - Self-attention allows modeling long-range dependencies
2. **Residual connections are critical** - Enable training deep networks
3. **Pre-training + Fine-tuning works** - Transfer learning is highly effective
4. **Implementation details matter** - Small choices (dropout, normalization order) impact results
5. **Start simple, iterate** - Build complexity gradually for better understanding

---

## ğŸ¤ Contributing

Feel free to:
- Open issues for bugs or questions
- Submit pull requests for improvements
- Share your own experiments and results

---

## ğŸ“„ License

This project is for educational purposes. Model architecture follows the GPT-2 paper by OpenAI.

---

## ğŸ™ Acknowledgments

- OpenAI for the GPT-2 architecture and paper
- PyTorch team for the excellent framework
- Jay Alammar for amazing visualizations
- The open-source ML community

---

<p align="center">
  <b>â­ If you found this helpful, please star the repository! â­</b>
</p>

<p align="center">
  Made with â¤ï¸ and lots of â˜• during my deep learning journey
</p>
