{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34710b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e0f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/the-verdict.txt',\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_data=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cf4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89aa4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text=tokenizer.encode(raw_data)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35502746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 367, 2885, 1464]--->1807\n"
     ]
    }
   ],
   "source": [
    "context_length=4\n",
    "x=enc_text[:context_length]\n",
    "y=enc_text[context_length]\n",
    "print(f\"{x}--->{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc2e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --->  H\n",
      "I H ---> AD\n",
      "I HAD --->  always\n",
      "I HAD always --->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,context_length+1):\n",
    "    context=enc_text[:i]\n",
    "    desired=enc_text[i]\n",
    "    print(tokenizer.decode(context),\"--->\",tokenizer.decode([desired]))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a9cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e460fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a16ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "targets\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader=create_dataloader_v1(raw_data,batch_size=8,max_length=4,stride=1,shuffle=False)\n",
    "data_iter=iter (dataloader)\n",
    "inputs,target=next(data_iter)\n",
    "print(\"INPUTS\\n\",inputs)\n",
    "print(\"targets\\n\",target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e4b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fbd07a",
   "metadata": {},
   "source": [
    ">token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3294c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  1.3337,  0.0771, -0.0522],\n",
      "        [ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [-1.3152, -0.0677, -0.1350,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        ...,\n",
      "        [ 0.5871, -0.0572, -1.1628,  ..., -0.6887, -0.7364,  0.4479],\n",
      "        [ 0.4438,  0.7411,  1.1263,  ...,  1.2091,  0.6781,  0.3331],\n",
      "        [-0.2537,  0.1446,  0.7203,  ..., -0.2134,  0.2144,  0.3006]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vocab_size=50257\n",
    "output_dim=256\n",
    "torch.manual_seed(123)  #random value alwase be constant when it is run \n",
    "embedding_layer=torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d15058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [ 0.5229,  0.1553,  0.5247,  ..., -0.2004,  0.8093, -0.6667],\n",
      "        [-0.3162,  1.2700, -0.0903,  ..., -0.4098,  0.4978, -0.3721],\n",
      "        [ 0.5229,  0.1553,  0.5247,  ..., -0.2004,  0.8093, -0.6667],\n",
      "        [ 0.2579,  0.3420, -0.8168,  ..., -0.4840, -0.2713, -0.0774],\n",
      "        [-0.3162,  1.2700, -0.0903,  ..., -0.4098,  0.4978, -0.3721]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids=torch.tensor([1,4,5,4,3,5])\n",
    "print(embedding_layer(input_ids)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "badd6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(raw_data,batch_size=8,max_length=max_length,stride=1,shuffle=False)\n",
    "data_iter=iter (dataloader)\n",
    "inputs,target=next(data_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fbaf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "shape\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUTS\\n\",inputs)\n",
    "print(\"shape\\n\",inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834c269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bddd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings=embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1998309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length=max_length\n",
    "pos_embedding_layer=torch.nn.Embedding(context_length,output_dim)\n",
    "pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bbaed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563bf27",
   "metadata": {},
   "source": [
    "# IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e34e0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "351286e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "267ee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10c8f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out),\n",
    "requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out),\n",
    "requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e34b7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)\n",
    "\n",
    "print(W_key)\n",
    "\n",
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60aa867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2=x_2@W_query\n",
    "key_2=x_2@W_key\n",
    "value_2=x_2@W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3622f2d",
   "metadata": {},
   "source": [
    ">for all input \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07335384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7362d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa9ba654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights = torch.softmax(attn_scores/ d_k**0.5, dim=-1)\n",
    "print(attn_weights)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31c40c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8346, 4.9351],\n",
      "        [2.5091, 6.7530],\n",
      "        [2.4766, 6.6657],\n",
      "        [1.3772, 3.7065],\n",
      "        [1.1948, 3.2184],\n",
      "        [1.7832, 4.7978]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_vec = attn_scores @ values\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261655d",
   "metadata": {},
   "source": [
    "# lets wrap the entire attention mechanism in a class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bdcca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d061d",
   "metadata": {},
   "source": [
    "\t•\tsuper() returns a special object that represents the parent class.\n",
    "\t•\tThen super().__init__() calls the parent class’s __init__.\n",
    "\t•\tThis avoids writing the parent class name manually \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02283fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5805f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#GPT-2's original implementation doesn't use bias for Q/K/V\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7e1af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#likely used 789 to get different initial weights than sa_v1 (which used seed 123), so the outputs would differ.\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f12cb",
   "metadata": {},
   "source": [
    "# masked attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39f68fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2705, 1.8524,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2544, 1.8284, 1.7877,   -inf,   -inf,   -inf],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925,   -inf,   -inf],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707,   -inf],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f7b87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8dc21",
   "metadata": {},
   "source": [
    "# dropout implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "476fc75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9329b8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.dropout(example,p=0.5,train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04306b35",
   "metadata": {},
   "source": [
    "During training, dropout is active. During inference, dropout is off. If we didn't scale, the model would see different magnitude values at train vs test time, breaking predictions.\n",
    "\n",
    "With scaling, no adjustment is needed at inference — the expected values match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "583215da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5052, 0.7582, 0.7366, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5677, 0.5587, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4727, 0.0000, 0.3639, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4183, 0.4097, 0.2839, 0.2178, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d4ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a755013",
   "metadata": {},
   "source": [
    "# batch demonstration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85a6804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a52ea0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628ae6b",
   "metadata": {},
   "source": [
    "# multi_head   attention  with weights split technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e1876",
   "metadata": {},
   "source": [
    ">d_in=input enbedding token dimension \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fb0f0",
   "metadata": {},
   "source": [
    "What is register_buffer?\n",
    "\n",
    "register_buffer is a method in nn.Module that registers a tensor as a buffer in the module.\n",
    "Buffers are tensors that are not learnable parameters (i.e., they are not updated during backpropagation) but are part of the model and saved with it (e.g., during state_dict()).\n",
    "Why use register_buffer?\n",
    "\n",
    "The mask is a fixed tensor that does not require gradients, so it is registered as a buffer instead of a parameter.\n",
    "This ensures the mask is moved to the correct device (CPU/GPU) along with the model, and it is included in the model's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca6df3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"sqb\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "  #self.mask.bool(): Converts the mask to a boolean tensor, where 1 becomes True and 0 becomes False.\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        #masked_fill:Replaces the positions where the mask is True with -torch.inf (negative infinity), effectively preventing attention to those positions.\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    " \n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "   \n",
    "    # This means the context vector has the same \n",
    "    # number of tokens as the input, but it now contains aggregated information from all attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29a65c",
   "metadata": {},
   "source": [
    "Why No Transpose of values?\n",
    "In matrix multiplication, the dot product is performed between the last dimension of the first tensor and the second-to-last dimension of the second tensor. In this case:\n",
    "\n",
    "attn_weights has shape (batch_size, num_heads, num_tokens, num_tokens).\n",
    "values has shape (batch_size, num_heads, num_tokens, head_dim).\n",
    "The dot product attn_weights @ values works as follows:\n",
    "\n",
    "For each batch_size and num_heads, the num_tokens dimension of attn_weights (last dimension) is multiplied with the num_tokens dimension of values (second-to-last dimension).\n",
    "This results in a tensor of shape (batch_size, num_heads, num_tokens, head_dim).\n",
    "Since the dimensions already align for the dot product, there is no need to transpose values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5fa9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5dfed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a370f5",
   "metadata": {},
   "source": [
    "##  A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e45c9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e558fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4cadcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init(self,config):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(num_embeddings=config[\"vocab_size\"],embedding_dim=config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.dropout=nn.Dropout(p=config[\"drop_rate\"])\n",
    "\n",
    "    # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec70b6",
   "metadata": {},
   "source": [
    "# CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "982df416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)#keepdim make seperate mean of each separate \n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bc0c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f0dcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb856b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Layer normalization (LayerNorm) is applied before each of these two components, and\n",
    "dropout is applied after them to regularize the model and prevent overfitting. \n",
    "\n",
    "This is also known as Pre-LayerNorm. \n",
    "\n",
    "Older architectures, such as the original transformer model,\n",
    "applied layer normalization after the self-attention and feed-forward networks instead,\n",
    "known as Post-LayerNorm, which often leads to worse training dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c08185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e81a00",
   "metadata": {},
   "source": [
    "As we can see from the code output, the transformer block maintains the input dimensions in its output, indicating that the transformer architecture processes sequences of data without altering their shape throughout the network.\n",
    "\n",
    "The preservation of shape throughout the transformer block architecture is not incidental but a crucial aspect of its design.\n",
    "\n",
    "This design enables its effective application across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector, maintaining a one-to-one relationship.\n",
    "\n",
    "However, the output is a context vector that encapsulates information from the entire input sequence.\n",
    "\n",
    "This means that while the physical dimensions of the sequence (length and feature size) remain unchanged as it passes through the transformer block, the content of each output vector is re-encoded to integrate contextual information from across the entire input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d625e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a940510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input indices:\n",
      " tensor([[  40,  367, 2885, 1464],\n",
      "        [ 367, 2885, 1464, 1807]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.7933,  0.2575,  0.7163,  ..., -0.0135,  0.9077, -0.4567],\n",
      "         [ 0.4344,  0.1495,  0.1858,  ...,  0.1104, -0.2476,  0.2734],\n",
      "         [ 0.2983,  0.3829,  1.0709,  ...,  0.1644, -1.1137, -0.0142],\n",
      "         [-0.3540,  0.4960,  0.6954,  ...,  1.4487,  0.0375, -0.7352]],\n",
      "\n",
      "        [[-0.3793, -0.6805,  0.0704,  ..., -0.3442, -0.2797,  0.2062],\n",
      "         [-0.3773,  0.2183,  0.4968,  ...,  0.2187, -0.5051,  0.0812],\n",
      "         [ 1.3646,  0.7179, -0.1557,  ...,  0.8394,  0.1912, -1.2974],\n",
      "         [-0.5940, -0.5886,  0.3251,  ...,  0.8174, -0.4465, -0.2724]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a batch of integer token indices for GPTModel\n",
    "# Example: use the first 4 tokens from enc_text for each batch\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "input_indices = torch.tensor([enc_text[:seq_len], enc_text[1:seq_len+1]])  # shape: (2, 4)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(input_indices)\n",
    "print(\"Input indices:\\n\", input_indices)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5969bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "724d7d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18bff8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b4a5b",
   "metadata": {},
   "source": [
    " >In PyTorch, model parameters are typically stored as 32-bit floating-point numbers (float32), and each float32 value takes up 4 bytes of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab8a5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a83ca38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2a5e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1318667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f3b92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3fc33",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "\n",
    "This is so that more readers will be able to follow and execute the code examples on their laptop computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "817e27c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "911373c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"datasets/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64f8deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "762f5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af7545cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37168187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abc3e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f22857e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())      # should be True\n",
    "print(torch.backends.mps.is_built())           # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87d20c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training loss: 10.987582948472765\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7d740f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b0b1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fb68c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "545053c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.817, Val loss 9.924\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.332\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.619, Val loss 7.042\n",
      "Ep 2 (Step 000015): Train loss 6.046, Val loss 6.596\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.524, Val loss 6.508\n",
      "Ep 3 (Step 000025): Train loss 5.369, Val loss 6.378\n",
      "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
      "Ep 4 (Step 000030): Train loss 4.830, Val loss 6.263\n",
      "Ep 4 (Step 000035): Train loss 4.586, Val loss 6.285\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 3.879, Val loss 6.130\n",
      "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
      "Ep 6 (Step 000045): Train loss 3.530, Val loss 6.183\n",
      "Ep 6 (Step 000050): Train loss 2.960, Val loss 6.123\n",
      "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
      "Ep 7 (Step 000055): Train loss 2.832, Val loss 6.150\n",
      "Ep 7 (Step 000060): Train loss 2.104, Val loss 6.133\n",
      "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
      "Ep 8 (Step 000065): Train loss 1.691, Val loss 6.186\n",
      "Ep 8 (Step 000070): Train loss 1.391, Val loss 6.230\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.059, Val loss 6.251\n",
      "Ep 9 (Step 000080): Train loss 0.800, Val loss 6.278\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
      "Ep 10 (Step 000085): Train loss 0.569, Val loss 6.373\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 0.52 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8fc66798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a468f8c",
   "metadata": {},
   "source": [
    "# DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e9ad318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ecd9a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102ab9a",
   "metadata": {},
   "source": [
    "# saving the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6f872484",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "11baba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
